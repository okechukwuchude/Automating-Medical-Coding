{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import stanza\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# Build an English pipeline\n",
    "stanza.download('en', package='mimic', processors={'ner': 'i2b2'}) # download English model\n",
    "nlp = stanza.Pipeline('en', package='mimic', processors={'ner': 'i2b2'}) # initialize English neural pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process text using Stanza\n",
    "def process_text_with_stanza(text):\n",
    "    doc = nlp(text)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable to store all extracted texts\n",
    "extracted_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract information from JSON files\n",
    "def extract_info_from_json(json_file_path, hadm_id_set):\n",
    "    global extracted_texts\n",
    "\n",
    "    # Open the JSON file\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        # Load JSON data from the file\n",
    "        data = json.load(file)\n",
    "\n",
    "        # Extract 'hadm_id' and 'comment' from the JSON data\n",
    "        hadm_id = data.get('hadm_id', None)\n",
    "        comment = data.get('comment', None)\n",
    "\n",
    "        # If 'hadm_id' is not found, print a warning message\n",
    "        if hadm_id is None:\n",
    "            print(f\"Warning: 'hadm_id' not found in {json_file_path}\")\n",
    "            return None\n",
    "\n",
    "        # Add 'hadm_id' to the set\n",
    "        hadm_id_set.add(hadm_id)\n",
    "\n",
    "        # Extract 'notes' from the JSON data\n",
    "        notes = data.get('notes', [])\n",
    "\n",
    "        # Iterate through each note\n",
    "        for note in notes:\n",
    "            note_info = {}  # Initialize a dictionary to store note information\n",
    "\n",
    "            # Add 'hadm_id' to the note information\n",
    "            note_info['hadm_id'] = hadm_id\n",
    "\n",
    "            # Extract information from the note\n",
    "            note_info['note_id'] = note.get('note_id', None)\n",
    "            note_info['category'] = note.get('category', None)\n",
    "            note_info['description'] = note.get('description', None)\n",
    "\n",
    "            # Extract annotations from the note\n",
    "            annotations = note.get('annotations', [])\n",
    "            annotations_info = []  # Initialize a list to store annotation information\n",
    "\n",
    "            # Iterate through each annotation in the note\n",
    "            for annotation in annotations:\n",
    "                annotation_info = {}  # Initialize a dictionary to store annotation information\n",
    "\n",
    "                # Extract information from the annotation\n",
    "                annotation_info['begin'] = annotation.get('begin', None)\n",
    "                annotation_info['end'] = annotation.get('end', None)\n",
    "                annotation_info['code'] = annotation.get('code', None)\n",
    "                annotation_info['code_system'] = annotation.get('code_system', None)\n",
    "                annotation_info['description'] = annotation.get('description', None)\n",
    "                annotation_info['type'] = annotation.get('type', None)\n",
    "                annotation_info['covered_text'] = annotation.get('covered_text', None)\n",
    "\n",
    "                annotations_info.append(annotation_info)  # Append annotation information to the list\n",
    "\n",
    "            note_info['annotations'] = annotations_info  # Add annotations information to the note\n",
    "            note_info['text'] = note.get('text', None)\n",
    "\n",
    "            # Process text with Stanza\n",
    "            if note_info['text']:\n",
    "                processed_text = process_text_with_stanza(note_info['text'])\n",
    "                note_info['processed_text'] = processed_text\n",
    "\n",
    "                # Extract sentence information and generate labels\n",
    "                sentence_info = []\n",
    "                for sent in processed_text.sentences:\n",
    "                    sentence_start = sent.words[0].start_char\n",
    "                    sentence_end = sent.words[-1].end_char\n",
    "                    include_sentence = False\n",
    "\n",
    "                    for annotation in note_info['annotations']:\n",
    "                        begin = annotation['begin']\n",
    "                        end = annotation['end']\n",
    "                        covered_text = annotation['covered_text']\n",
    "\n",
    "                        # Check if the sentence contains the covered text\n",
    "                        if sentence_start <= begin and sentence_end >= end:\n",
    "                            include_sentence = True\n",
    "                            tokens = [word.text for word in sent.words]\n",
    "                            start_token_idx, end_token_idx = find_token_indices(tokens, begin, end, covered_text)\n",
    "                            \n",
    "                            # Generate labels for the covered text\n",
    "                            labels = generate_labels(tokens, [annotation], start_token_idx, end_token_idx)\n",
    "                            break\n",
    "\n",
    "                    if include_sentence:\n",
    "                        sentence_info.append({\n",
    "                            'sentence_id': sent.index,\n",
    "                            'words': tokens,\n",
    "                            'labels': labels\n",
    "                        })\n",
    "\n",
    "                note_info['sentence_info'] = sentence_info\n",
    "\n",
    "            extracted_texts.append(note_info)  # Append note information to the global variable\n",
    "\n",
    "# Function to find the token indices corresponding to the annotation\n",
    "def find_token_indices(tokens, begin, end, covered_text):\n",
    "    char_counter = 0\n",
    "    start_token_idx = None\n",
    "    end_token_idx = None\n",
    "\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if char_counter == begin:\n",
    "            start_token_idx = idx\n",
    "        if char_counter == end:\n",
    "            end_token_idx = idx\n",
    "            break\n",
    "        char_counter += len(token) + 1  # Add 1 for the space after each token\n",
    "\n",
    "    return start_token_idx, end_token_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_indices(tokens, begin, end):\n",
    "    word_indices = find_word_indices(tokens, begin, end)\n",
    "\n",
    "    \n",
    "    for idx, token in enumerate(tokens):\n",
    "        if token.startchar == begin or (token.startchar < begin and token.endchar > begin):\n",
    "            word_indices.append(idx)\n",
    "        if token.endchar == end or (token.startchar < end and token.endchar > end):\n",
    "            word_indices.append(idx)\n",
    "    \n",
    "    return word_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(words, annotations, start_token_idx, end_token_idx):\n",
    "    labels = ['O'] * len(words)\n",
    "\n",
    "    for annotation in annotations:\n",
    "        code = annotation['code']\n",
    "\n",
    "        # Update labels for the matched words\n",
    "        if start_token_idx is not None and end_token_idx is not None:\n",
    "            labels[start_token_idx] = f'B-{code}'\n",
    "            for idx in range(start_token_idx + 1, end_token_idx + 1):\n",
    "                labels[idx] = f'I-{code}'\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search for JSON files in a given folder and its subfolders\n",
    "\n",
    "def search_files(folder_path):\n",
    "    # Set to store unique hadm_id values\n",
    "    hadm_id_set = set()\n",
    "\n",
    "    # Recursively search for JSON files in the folder and its subfolders\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.json'):\n",
    "                # Construct the full path to the JSON file\n",
    "                json_file_path = os.path.join(root, filename)\n",
    "                print(\"Processing:\", json_file_path)\n",
    "                # Call extract_info_from_json function to extract information from the JSON file\n",
    "                extract_info_from_json(json_file_path, hadm_id_set)\n",
    "                # Print a separator after processing each file\n",
    "                print(\"=\" * 50)\n",
    "\n",
    "    # Print the count of unique hadm_id values\n",
    "    print(\"Total unique hadm_id count:\", len(hadm_id_set))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: C:\\Users\\okechukwu chude\\Documents\\NLP\\text extraction\\Automating-Medical-Coding\\2\\100197-ICD-9.json\n",
      "==================================================\n",
      "Total unique hadm_id count: 1\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to the main folder containing subfolders with JSON files\n",
    "main_folder_path = r\"C:\\Users\\okechukwu chude\\Documents\\NLP\\text extraction\\Automating-Medical-Coding\\2\"\n",
    "\n",
    "# Call the search_files function to start searching for JSON files in the main folder and its subfolders\n",
    "search_files(main_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     hadm_id  note_id  sentence_id         word label\n",
      "0     100197    25762            6      History     O\n",
      "1     100197    25762            6           of     O\n",
      "2     100197    25762            6      Present     O\n",
      "3     100197    25762            6      Illness     O\n",
      "4     100197    25762            6            :     O\n",
      "..       ...      ...          ...          ...   ...\n",
      "168   100197    25762           37     titrated     O\n",
      "169   100197    25762           37           to     O\n",
      "170   100197    25762           37  respiratory     O\n",
      "171   100197    25762           37         rate     O\n",
      "172   100197    25762           37            .     O\n",
      "\n",
      "[173 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to create a DataFrame from the extracted sentence information\n",
    "def create_dataframe(extracted_texts):\n",
    "    data = []\n",
    "\n",
    "    for note_info in extracted_texts:\n",
    "        if 'sentence_info' in note_info:\n",
    "            for sentence_info in note_info['sentence_info']:\n",
    "                words = sentence_info['words']\n",
    "                labels = sentence_info['labels']\n",
    "\n",
    "                for word, label in zip(words, labels):\n",
    "                    data.append({\n",
    "                        'hadm_id': note_info['hadm_id'],\n",
    "                        'note_id': note_info['note_id'],\n",
    "                        'sentence_id': sentence_info['sentence_id'],\n",
    "                        'word': word,\n",
    "                        'label': label\n",
    "                    })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# After processing the JSON files and populating the extracted_texts list\n",
    "df = create_dataframe(extracted_texts)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
