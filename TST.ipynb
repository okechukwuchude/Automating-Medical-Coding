{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 373kB [00:00, 10.7MB/s]                    \n",
      "2024-03-17 14:52:44 INFO: Downloaded file to C:\\Users\\okechukwu chude\\stanza_resources\\resources.json\n",
      "2024-03-17 14:52:44 INFO: Downloading these customized packages for language: en (English)...\n",
      "====================================\n",
      "| Processor       | Package        |\n",
      "------------------------------------\n",
      "| tokenize        | mimic          |\n",
      "| pos             | mimic_charlm   |\n",
      "| lemma           | mimic_nocharlm |\n",
      "| depparse        | mimic_charlm   |\n",
      "| ner             | i2b2           |\n",
      "| pretrain        | mimic          |\n",
      "| backward_charlm | mimic          |\n",
      "| forward_charlm  | mimic          |\n",
      "====================================\n",
      "\n",
      "2024-03-17 14:52:44 INFO: File exists: C:\\Users\\okechukwu chude\\stanza_resources\\en\\tokenize\\mimic.pt\n",
      "2024-03-17 14:52:44 INFO: File exists: C:\\Users\\okechukwu chude\\stanza_resources\\en\\pos\\mimic_charlm.pt\n",
      "2024-03-17 14:52:44 INFO: File exists: C:\\Users\\okechukwu chude\\stanza_resources\\en\\lemma\\mimic_nocharlm.pt\n",
      "2024-03-17 14:52:44 INFO: File exists: C:\\Users\\okechukwu chude\\stanza_resources\\en\\depparse\\mimic_charlm.pt\n",
      "2024-03-17 14:52:44 INFO: File exists: C:\\Users\\okechukwu chude\\stanza_resources\\en\\ner\\i2b2.pt\n",
      "2024-03-17 14:52:44 INFO: File exists: C:\\Users\\okechukwu chude\\stanza_resources\\en\\pretrain\\mimic.pt\n",
      "2024-03-17 14:52:44 INFO: File exists: C:\\Users\\okechukwu chude\\stanza_resources\\en\\backward_charlm\\mimic.pt\n",
      "2024-03-17 14:52:44 INFO: File exists: C:\\Users\\okechukwu chude\\stanza_resources\\en\\forward_charlm\\mimic.pt\n",
      "2024-03-17 14:52:44 INFO: Finished downloading models and saved to C:\\Users\\okechukwu chude\\stanza_resources\n",
      "2024-03-17 14:52:44 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 373kB [00:00, 5.28MB/s]                    \n",
      "2024-03-17 14:52:45 INFO: Downloaded file to C:\\Users\\okechukwu chude\\stanza_resources\\resources.json\n",
      "2024-03-17 14:52:46 INFO: Loading these models for language: en (English):\n",
      "==============================\n",
      "| Processor | Package        |\n",
      "------------------------------\n",
      "| tokenize  | mimic          |\n",
      "| pos       | mimic_charlm   |\n",
      "| lemma     | mimic_nocharlm |\n",
      "| depparse  | mimic_charlm   |\n",
      "| ner       | i2b2           |\n",
      "==============================\n",
      "\n",
      "2024-03-17 14:52:46 INFO: Using device: cpu\n",
      "2024-03-17 14:52:46 INFO: Loading: tokenize\n",
      "2024-03-17 14:52:46 INFO: Loading: pos\n",
      "2024-03-17 14:52:47 INFO: Loading: lemma\n",
      "2024-03-17 14:52:47 INFO: Loading: depparse\n",
      "2024-03-17 14:52:47 INFO: Loading: ner\n",
      "2024-03-17 14:52:48 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import stanza\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# Build an English pipeline\n",
    "stanza.download('en', package='mimic', processors={'ner': 'i2b2'}) # download English model\n",
    "nlp = stanza.Pipeline('en', package='mimic', processors={'ner': 'i2b2'}) # initialize English neural pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process text using Stanza\n",
    "def process_text_with_stanza(text):\n",
    "    doc = nlp(text)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable to store all extracted texts\n",
    "extracted_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract information from JSON files\n",
    "def extract_info_from_json(json_file_path, hadm_id_set):\n",
    "    global extracted_texts\n",
    "\n",
    "    # Open the JSON file\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        # Load JSON data from the file\n",
    "        data = json.load(file)\n",
    "\n",
    "        # Extract 'hadm_id' and 'comment' from the JSON data\n",
    "        hadm_id = data.get('hadm_id', None)\n",
    "        comment = data.get('comment', None)\n",
    "\n",
    "        # If 'hadm_id' is not found, print a warning message\n",
    "        if hadm_id is None:\n",
    "            print(f\"Warning: 'hadm_id' not found in {json_file_path}\")\n",
    "            return None\n",
    "\n",
    "        # Add 'hadm_id' to the set\n",
    "        hadm_id_set.add(hadm_id)\n",
    "\n",
    "        # Extract 'notes' from the JSON data\n",
    "        notes = data.get('notes', [])\n",
    "\n",
    "        # Iterate through each note\n",
    "        for note in notes:\n",
    "            note_info = {}  # Initialize a dictionary to store note information\n",
    "\n",
    "            # Add 'hadm_id' to the note information\n",
    "            note_info['hadm_id'] = hadm_id\n",
    "\n",
    "            # Extract information from the note\n",
    "            note_info['note_id'] = note.get('note_id', None)\n",
    "            note_info['category'] = note.get('category', None)\n",
    "            note_info['description'] = note.get('description', None)\n",
    "\n",
    "            # Extract annotations from the note\n",
    "            annotations = note.get('annotations', [])\n",
    "            annotations_info = []  # Initialize a list to store annotation information\n",
    "\n",
    "            # Iterate through each annotation in the note\n",
    "            for annotation in annotations:\n",
    "                annotation_info = {}  # Initialize a dictionary to store annotation information\n",
    "\n",
    "                # Extract information from the annotation\n",
    "                annotation_info['begin'] = annotation.get('begin', None)\n",
    "                annotation_info['end'] = annotation.get('end', None)\n",
    "                annotation_info['code'] = annotation.get('code', None)\n",
    "                annotation_info['code_system'] = annotation.get('code_system', None)\n",
    "                annotation_info['description'] = annotation.get('description', None)\n",
    "                annotation_info['type'] = annotation.get('type', None)\n",
    "                annotation_info['covered_text'] = annotation.get('covered_text', None)\n",
    "\n",
    "                annotations_info.append(annotation_info)  # Append annotation information to the list\n",
    "\n",
    "            note_info['annotations'] = annotations_info  # Add annotations information to the note\n",
    "            note_info['text'] = note.get('text', None)\n",
    "\n",
    "            # Process text with Stanza\n",
    "            if note_info['text']:\n",
    "                processed_text = process_text_with_stanza(note_info['text'])\n",
    "                note_info['processed_text'] = processed_text\n",
    "\n",
    "                # Extract sentence information and generate labels\n",
    "                sentence_info = []\n",
    "                for sent in processed_text.sentences:\n",
    "                    sentence_start = sent.words[0].start_char\n",
    "                    sentence_end = sent.words[-1].end_char\n",
    "                    include_sentence = False\n",
    "\n",
    "                    for annotation in note_info['annotations']:\n",
    "                        begin = annotation['begin']\n",
    "                        end = annotation['end']\n",
    "                        covered_text = annotation['covered_text']\n",
    "\n",
    "                        # Check if the sentence contains the covered text\n",
    "                        if sentence_start <= begin and sentence_end >= end:\n",
    "                            include_sentence = True\n",
    "                            tokens = [word.text for word in sent.words]\n",
    "                            start_token_idx, end_token_idx = find_token_indices(tokens, begin, end, covered_text)\n",
    "                            \n",
    "                            # Generate labels for the covered text\n",
    "                            labels = generate_labels(tokens, [annotation], start_token_idx, end_token_idx)\n",
    "                            break\n",
    "\n",
    "                    if include_sentence:\n",
    "                        sentence_info.append({\n",
    "                            'sentence_id': sent.index,\n",
    "                            'words': tokens,\n",
    "                            'labels': labels\n",
    "                        })\n",
    "\n",
    "                note_info['sentence_info'] = sentence_info\n",
    "\n",
    "            extracted_texts.append(note_info)  # Append note information to the global variable\n",
    "\n",
    "# Function to find the token indices corresponding to the annotation\n",
    "def find_token_indices(tokens, begin, end, covered_text):\n",
    "    char_counter = 0\n",
    "    start_token_idx = None\n",
    "    end_token_idx = None\n",
    "\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if char_counter == begin:\n",
    "            start_token_idx = idx\n",
    "        if char_counter == end:\n",
    "            end_token_idx = idx\n",
    "            break\n",
    "        char_counter += len(token) + 1  # Add 1 for the space after each token\n",
    "\n",
    "    return start_token_idx, end_token_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_indices(tokens, begin, end):\n",
    "    # Create an empty list to store the indices of words found within the given range\n",
    "    word_indices = []\n",
    "\n",
    "    # Iterate through each token in the list of tokens\n",
    "    for idx, token in enumerate(tokens):\n",
    "        # Check if the start character of the token matches the beginning of the range\n",
    "        # or if the token spans the beginning of the range\n",
    "        if token.startchar == begin or (token.startchar < begin and token.endchar > begin):\n",
    "            # If it matches, add the index of the token to the list of word indices\n",
    "            word_indices.append(idx)\n",
    "        # Check if the end character of the token matches the end of the range\n",
    "        # or if the token spans the end of the range\n",
    "        if token.endchar == end or (token.startchar < end and token.endchar > end):\n",
    "            # If it matches, add the index of the token to the list of word indices\n",
    "            word_indices.append(idx)\n",
    "            break\n",
    "    \n",
    "    # Return the list of word indices found within the given range\n",
    "    return word_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(words, annotations, start_token_idx, end_token_idx):\n",
    "    labels = ['O'] * len(words)\n",
    "\n",
    "    for annotation in annotations:\n",
    "        code = annotation['code']\n",
    "\n",
    "        # Update labels for the matched words\n",
    "        if start_token_idx is not None and end_token_idx is not None:\n",
    "            labels[start_token_idx] = f'B-{code}'\n",
    "            for idx in range(start_token_idx + 1, end_token_idx + 1):\n",
    "                labels[idx] = f'I-{code}'\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search for JSON files in a given folder and its subfolders\n",
    "\n",
    "def search_files(folder_path):\n",
    "    # Set to store unique hadm_id values\n",
    "    hadm_id_set = set()\n",
    "\n",
    "    # Recursively search for JSON files in the folder and its subfolders\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.json'):\n",
    "                # Construct the full path to the JSON file\n",
    "                json_file_path = os.path.join(root, filename)\n",
    "                print(\"Processing:\", json_file_path)\n",
    "                # Call extract_info_from_json function to extract information from the JSON file\n",
    "                extract_info_from_json(json_file_path, hadm_id_set)\n",
    "                # Print a separator after processing each file\n",
    "                print(\"=\" * 50)\n",
    "\n",
    "    # Print the count of unique hadm_id values\n",
    "    print(\"Total unique hadm_id count:\", len(hadm_id_set))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: C:\\Users\\okechukwu chude\\Documents\\NLP\\text extraction\\Automating-Medical-Coding\\2\\100197-ICD-9.json\n",
      "==================================================\n",
      "Processing: C:\\Users\\okechukwu chude\\Documents\\NLP\\text extraction\\Automating-Medical-Coding\\2\\ICD-10\\1.0\\101525-ICD-10.json\n",
      "==================================================\n",
      "Total unique hadm_id count: 2\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to the main folder containing subfolders with JSON files\n",
    "main_folder_path = r\"C:\\Users\\okechukwu chude\\Documents\\NLP\\text extraction\\Automating-Medical-Coding\\2\"\n",
    "\n",
    "# Call the search_files function to start searching for JSON files in the main folder and its subfolders\n",
    "search_files(main_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      hadm_id  note_id  sentence_id     word label\n",
      "0      100197    25762            6  History     O\n",
      "1      100197    25762            6       of     O\n",
      "2      100197    25762            6  Present     O\n",
      "3      100197    25762            6  Illness     O\n",
      "4      100197    25762            6        :     O\n",
      "...       ...      ...          ...      ...   ...\n",
      "1627   101525  1071963            4     pain     O\n",
      "1628   101525  1071963            4        .     O\n",
      "1629   101525   240524            0    Sinus     O\n",
      "1630   101525   240524            0   rhythm     O\n",
      "1631   101525   240524            0        .     O\n",
      "\n",
      "[1632 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to create a DataFrame from the extracted sentence information\n",
    "def create_dataframe(extracted_texts):\n",
    "    data = []\n",
    "\n",
    "    for note_info in extracted_texts:\n",
    "        if 'sentence_info' in note_info:\n",
    "            for sentence_info in note_info['sentence_info']:\n",
    "                words = sentence_info['words']\n",
    "                labels = sentence_info['labels']\n",
    "\n",
    "                for word, label in zip(words, labels):\n",
    "                    data.append({\n",
    "                        'hadm_id': note_info['hadm_id'],\n",
    "                        'note_id': note_info['note_id'],\n",
    "                        'sentence_id': sentence_info['sentence_id'],\n",
    "                        'word': word,\n",
    "                        'label': label\n",
    "                    })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "df = create_dataframe(extracted_texts)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "O    1632\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_counts = df['label'].value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
