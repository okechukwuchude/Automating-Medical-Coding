{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_note_text(notes_map: Dict[int, str], admission: Dict) -> Dict:\n",
    "    \"\"\"Inject text in-place into admission\"\"\"\n",
    "    for note in admission[\"notes\"]:\n",
    "        note_id = note.get(\"note_id\")  # Get note_id from note\n",
    "        if note_id is not None:\n",
    "            text = notes_map.get(note_id)  # Get text from notes_map\n",
    "            if text is not None:\n",
    "                note[\"text\"] = text\n",
    "                for annotation in note.get(\"annotations\", []):\n",
    "                    begin = annotation.get(\"begin\", 0)\n",
    "                    end = annotation.get(\"end\", 0)\n",
    "                    annotation[\"covered_text\"] = text[begin:end]\n",
    "            else:\n",
    "                # If you want to print a warning message instead of using logger:\n",
    "                print(f\"No text found for note_id: {note_id}\")\n",
    "        else:\n",
    "            # If you want to print a warning message instead of using logger:\n",
    "            print(\"No note_id found in note\")\n",
    "\n",
    "    return admission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _make_out_path(json_file: Path, input_dir: Path, out_dir: Path) -> Path:\n",
    "    \"\"\"Generate output path for injected JSON file\"\"\"\n",
    "    prefix_len = len(input_dir.parts)\n",
    "    return out_dir.joinpath(*json_file.parts[prefix_len:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_and_persist(notes_map: Dict[int, str], data_dir: Path, out_dir: Path):\n",
    "    \"\"\"Inject text into admission and persist in out_dir if provided\"\"\"\n",
    "    if out_dir:\n",
    "        print(f\"Injecting text and persisting to {out_dir.absolute()}\")\n",
    "    else:\n",
    "        print(\"Injecting text in place\")\n",
    "\n",
    "    # Iterate through JSON files in data_dir\n",
    "    for json_file in data_dir.glob(\"**/*.json\"):\n",
    "        with open(json_file, \"r\", encoding=\"utf8\") as ifp:\n",
    "            admission = inject_note_text(notes_map, json.load(ifp))\n",
    "\n",
    "        # Create output path\n",
    "        if out_dir:\n",
    "            out_path = _make_out_path(json_file, data_dir, out_dir)\n",
    "            out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        else:\n",
    "            out_path = json_file\n",
    "\n",
    "        # Write injected admission to output file\n",
    "        with open(out_path, \"w\", encoding=\"utf8\") as ofp:\n",
    "            json.dump(admission, ofp, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_notes_map(noteevents: Path) -> Dict[int, str]:\n",
    "    \"\"\"Build mapping from note_id to text from NOTEEVENTS.csv\"\"\"\n",
    "    print(f\"Loading {noteevents}\")\n",
    "    id_text_map = dict()\n",
    "    csv.field_size_limit(1024 * 1024 * 1024)\n",
    "    with open(noteevents, \"r\", encoding=\"utf8\") as ifp:\n",
    "        reader = csv.reader(ifp)\n",
    "        # Skip header\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            note_id, text = int(row[0]), row[10]\n",
    "            id_text_map[note_id] = text\n",
    "    return id_text_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(noteevents_path: Path, data_dir: Path, out_dir: Path = None):\n",
    "    # Build mapping from note_id to text\n",
    "    notes_map = build_notes_map(noteevents_path)\n",
    "    # Inject text into admissions and persist if out_dir provided\n",
    "    inject_and_persist(notes_map, data_dir, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading C:\\Users\\okechukwu chude\\Documents\\NLP\\text extraction\\Automating-Medical-Coding\\NOTEEVENTS.csv\n",
      "Injecting text and persisting to C:\\Users\\okechukwu chude\\Documents\\NLP\\text extraction\\Automating-Medical-Coding\\with_text\\gold\n",
      "No text found for note_id: 45238\n",
      "No text found for note_id: 1188351\n",
      "No text found for note_id: 1187104\n",
      "No text found for note_id: 1188743\n",
      "No text found for note_id: 1188269\n",
      "No text found for note_id: 1187310\n",
      "No text found for note_id: 1188081\n",
      "No text found for note_id: 1188815\n",
      "No text found for note_id: 37661\n",
      "No text found for note_id: 709614\n",
      "No text found for note_id: 709437\n",
      "No text found for note_id: 709357\n",
      "No text found for note_id: 709697\n",
      "No text found for note_id: 445921\n",
      "No text found for note_id: 445953\n",
      "No text found for note_id: 35602\n",
      "No text found for note_id: 38450\n",
      "No text found for note_id: 1199478\n",
      "No text found for note_id: 38312\n",
      "No text found for note_id: 42388\n",
      "No text found for note_id: 1251065\n",
      "No text found for note_id: 1251080\n",
      "No text found for note_id: 42674\n",
      "No text found for note_id: 36594\n",
      "No text found for note_id: 690868\n",
      "No text found for note_id: 690973\n",
      "No text found for note_id: 690909\n",
      "No text found for note_id: 690896\n",
      "No text found for note_id: 691198\n",
      "No text found for note_id: 41042\n",
      "No text found for note_id: 3457\n",
      "No text found for note_id: 435376\n",
      "No text found for note_id: 434386\n",
      "No text found for note_id: 435572\n",
      "No text found for note_id: 436012\n",
      "No text found for note_id: 434134\n",
      "No text found for note_id: 434774\n",
      "No text found for note_id: 1055968\n",
      "No text found for note_id: 433504\n",
      "No text found for note_id: 433799\n",
      "No text found for note_id: 50967\n",
      "No text found for note_id: 46059\n",
      "No text found for note_id: 38112\n",
      "No text found for note_id: 48918\n",
      "No text found for note_id: 53255\n",
      "No text found for note_id: 688805\n",
      "No text found for note_id: 688827\n",
      "No text found for note_id: 513928\n",
      "No text found for note_id: 37210\n",
      "No text found for note_id: 513744\n",
      "No text found for note_id: 513613\n",
      "No text found for note_id: 513754\n",
      "No text found for note_id: 3152\n",
      "No text found for note_id: 1253918\n",
      "No text found for note_id: 45784\n",
      "No text found for note_id: 634149\n",
      "No text found for note_id: 634428\n",
      "No text found for note_id: 634340\n",
      "No text found for note_id: 35665\n",
      "No text found for note_id: 554439\n",
      "No text found for note_id: 554202\n",
      "No text found for note_id: 554344\n",
      "No text found for note_id: 554242\n",
      "No text found for note_id: 40904\n",
      "No text found for note_id: 34877\n",
      "No text found for note_id: 57952\n",
      "No text found for note_id: 498478\n",
      "No text found for note_id: 498471\n",
      "No text found for note_id: 498662\n",
      "No text found for note_id: 498568\n",
      "No text found for note_id: 497834\n",
      "No text found for note_id: 497795\n",
      "No text found for note_id: 498085\n",
      "No text found for note_id: 498755\n",
      "No text found for note_id: 40230\n",
      "No text found for note_id: 1168841\n",
      "No text found for note_id: 42073\n",
      "No text found for note_id: 1221260\n",
      "No text found for note_id: 11299\n",
      "No text found for note_id: 53709\n",
      "No text found for note_id: 1236681\n",
      "No text found for note_id: 37218\n",
      "No text found for note_id: 45120\n",
      "No text found for note_id: 35302\n",
      "No text found for note_id: 674183\n",
      "No text found for note_id: 32090\n",
      "No text found for note_id: 49145\n",
      "No text found for note_id: 54862\n",
      "No text found for note_id: 46251\n",
      "No text found for note_id: 38406\n",
      "No text found for note_id: 16777\n",
      "No text found for note_id: 614344\n",
      "No text found for note_id: 615618\n",
      "No text found for note_id: 615611\n",
      "No text found for note_id: 615200\n",
      "No text found for note_id: 615672\n",
      "No text found for note_id: 615614\n",
      "No text found for note_id: 614290\n",
      "No text found for note_id: 37027\n",
      "No text found for note_id: 48799\n",
      "No text found for note_id: 372\n",
      "No text found for note_id: 54476\n",
      "No text found for note_id: 612314\n",
      "No text found for note_id: 611922\n",
      "No text found for note_id: 611940\n",
      "No text found for note_id: 36834\n",
      "No text found for note_id: 527977\n",
      "No text found for note_id: 529965\n",
      "No text found for note_id: 1126492\n",
      "No text found for note_id: 1127371\n",
      "No text found for note_id: 529470\n",
      "No text found for note_id: 1118442\n",
      "No text found for note_id: 1118443\n",
      "No text found for note_id: 527973\n",
      "No text found for note_id: 528191\n",
      "No text found for note_id: 527839\n",
      "No text found for note_id: 527964\n",
      "No text found for note_id: 36735\n",
      "No text found for note_id: 1085658\n",
      "No text found for note_id: 1085659\n",
      "No text found for note_id: 582646\n",
      "No text found for note_id: 582741\n",
      "No text found for note_id: 1085902\n",
      "No text found for note_id: 54285\n",
      "No text found for note_id: 683118\n",
      "No text found for note_id: 683082\n",
      "No text found for note_id: 682933\n",
      "No text found for note_id: 683081\n",
      "No text found for note_id: 683039\n",
      "No text found for note_id: 41970\n",
      "No text found for note_id: 14279\n",
      "No text found for note_id: 727745\n",
      "No text found for note_id: 727678\n",
      "No text found for note_id: 727759\n",
      "No text found for note_id: 727688\n",
      "No text found for note_id: 727523\n",
      "No text found for note_id: 1042300\n",
      "No text found for note_id: 1043209\n",
      "No text found for note_id: 1042299\n",
      "No text found for note_id: 1043370\n",
      "No text found for note_id: 1041884\n",
      "No text found for note_id: 35053\n",
      "No text found for note_id: 541876\n",
      "No text found for note_id: 38623\n",
      "No text found for note_id: 1187078\n",
      "No text found for note_id: 595410\n",
      "No text found for note_id: 595338\n",
      "No text found for note_id: 595514\n",
      "No text found for note_id: 595401\n",
      "No text found for note_id: 595179\n",
      "No text found for note_id: 594772\n",
      "No text found for note_id: 594797\n",
      "No text found for note_id: 23168\n",
      "No text found for note_id: 595170\n",
      "No text found for note_id: 595518\n",
      "No text found for note_id: 650376\n",
      "No text found for note_id: 650383\n",
      "No text found for note_id: 36132\n",
      "No text found for note_id: 33407\n",
      "No text found for note_id: 47049\n",
      "No text found for note_id: 4951\n",
      "No text found for note_id: 1131840\n",
      "No text found for note_id: 1132079\n",
      "No text found for note_id: 1131862\n",
      "No text found for note_id: 36738\n",
      "No text found for note_id: 478450\n",
      "No text found for note_id: 478501\n",
      "No text found for note_id: 58107\n",
      "No text found for note_id: 477572\n",
      "No text found for note_id: 477946\n",
      "No text found for note_id: 478006\n",
      "No text found for note_id: 477530\n",
      "No text found for note_id: 477772\n",
      "No text found for note_id: 46010\n",
      "No text found for note_id: 38851\n",
      "No text found for note_id: 1132733\n",
      "No text found for note_id: 1132636\n",
      "No text found for note_id: 1132652\n",
      "No text found for note_id: 717354\n",
      "No text found for note_id: 35339\n",
      "No text found for note_id: 717216\n",
      "No text found for note_id: 716990\n",
      "No text found for note_id: 716995\n",
      "No text found for note_id: 717539\n",
      "No text found for note_id: 50798\n",
      "No text found for note_id: 40745\n",
      "No text found for note_id: 28578\n",
      "No text found for note_id: 1255273\n",
      "No text found for note_id: 1255319\n",
      "No text found for note_id: 22294\n",
      "No text found for note_id: 1106893\n",
      "No text found for note_id: 604372\n",
      "No text found for note_id: 604246\n",
      "No text found for note_id: 604203\n",
      "No text found for note_id: 604135\n",
      "No text found for note_id: 604204\n",
      "No text found for note_id: 604202\n",
      "No text found for note_id: 1106833\n",
      "No text found for note_id: 52801\n",
      "No text found for note_id: 12466\n",
      "No text found for note_id: 40929\n",
      "No text found for note_id: 45238\n",
      "No text found for note_id: 37661\n",
      "No text found for note_id: 35602\n",
      "No text found for note_id: 38450\n",
      "No text found for note_id: 38312\n",
      "No text found for note_id: 42388\n",
      "No text found for note_id: 42674\n",
      "No text found for note_id: 36594\n",
      "No text found for note_id: 41042\n",
      "No text found for note_id: 3457\n",
      "No text found for note_id: 50967\n",
      "No text found for note_id: 46059\n",
      "No text found for note_id: 38112\n",
      "No text found for note_id: 48918\n",
      "No text found for note_id: 53255\n",
      "No text found for note_id: 37210\n",
      "No text found for note_id: 3152\n",
      "No text found for note_id: 45784\n",
      "No text found for note_id: 35665\n",
      "No text found for note_id: 40904\n",
      "No text found for note_id: 34877\n",
      "No text found for note_id: 57952\n",
      "No text found for note_id: 40230\n",
      "No text found for note_id: 42073\n",
      "No text found for note_id: 11299\n",
      "No text found for note_id: 53709\n",
      "No text found for note_id: 37218\n",
      "No text found for note_id: 45120\n",
      "No text found for note_id: 35302\n",
      "No text found for note_id: 32090\n",
      "No text found for note_id: 49145\n",
      "No text found for note_id: 54862\n",
      "No text found for note_id: 46251\n",
      "No text found for note_id: 38406\n",
      "No text found for note_id: 16777\n",
      "No text found for note_id: 37027\n",
      "No text found for note_id: 48799\n",
      "No text found for note_id: 372\n",
      "No text found for note_id: 54476\n",
      "No text found for note_id: 36834\n",
      "No text found for note_id: 36735\n",
      "No text found for note_id: 54285\n",
      "No text found for note_id: 41970\n",
      "No text found for note_id: 14279\n",
      "No text found for note_id: 35053\n",
      "No text found for note_id: 38623\n",
      "No text found for note_id: 23168\n",
      "No text found for note_id: 36132\n",
      "No text found for note_id: 33407\n",
      "No text found for note_id: 47049\n",
      "No text found for note_id: 4951\n",
      "No text found for note_id: 36738\n",
      "No text found for note_id: 46010\n",
      "No text found for note_id: 38851\n",
      "No text found for note_id: 35339\n",
      "No text found for note_id: 50798\n",
      "No text found for note_id: 40745\n",
      "No text found for note_id: 28578\n",
      "No text found for note_id: 22294\n",
      "No text found for note_id: 52801\n",
      "No text found for note_id: 12466\n",
      "No text found for note_id: 40929\n",
      "No text found for note_id: 42907\n",
      "No text found for note_id: 1218374\n",
      "No text found for note_id: 41817\n",
      "No text found for note_id: 1202412\n",
      "No text found for note_id: 1201961\n",
      "No text found for note_id: 1201619\n",
      "No text found for note_id: 53563\n",
      "No text found for note_id: 1242557\n",
      "No text found for note_id: 1242401\n",
      "No text found for note_id: 1242610\n",
      "No text found for note_id: 1242314\n",
      "No text found for note_id: 1242797\n",
      "No text found for note_id: 1243050\n",
      "No text found for note_id: 1242726\n",
      "No text found for note_id: 1242220\n",
      "No text found for note_id: 4475\n",
      "No text found for note_id: 1166574\n",
      "No text found for note_id: 1166707\n",
      "No text found for note_id: 1166852\n",
      "No text found for note_id: 1166838\n",
      "No text found for note_id: 1166886\n",
      "No text found for note_id: 683919\n",
      "No text found for note_id: 683829\n",
      "No text found for note_id: 36287\n",
      "No text found for note_id: 684008\n",
      "No text found for note_id: 683942\n",
      "No text found for note_id: 683956\n",
      "No text found for note_id: 683818\n",
      "No text found for note_id: 1085848\n",
      "No text found for note_id: 1085822\n",
      "No text found for note_id: 1085643\n",
      "No text found for note_id: 1138247\n",
      "No text found for note_id: 1138289\n",
      "No text found for note_id: 1138530\n",
      "No text found for note_id: 1138255\n",
      "No text found for note_id: 37218\n",
      "No text found for note_id: 1138845\n",
      "No text found for note_id: 1138261\n",
      "No text found for note_id: 1138690\n",
      "No text found for note_id: 1138407\n",
      "No text found for note_id: 1139418\n",
      "No text found for note_id: 1139417\n",
      "No text found for note_id: 36738\n",
      "No text found for note_id: 477734\n",
      "No text found for note_id: 477772\n",
      "No text found for note_id: 477572\n",
      "No text found for note_id: 478274\n",
      "No text found for note_id: 477530\n",
      "No text found for note_id: 478257\n",
      "No text found for note_id: 478501\n",
      "No text found for note_id: 478006\n",
      "No text found for note_id: 478450\n",
      "No text found for note_id: 477946\n",
      "No text found for note_id: 1093864\n",
      "No text found for note_id: 1093483\n",
      "No text found for note_id: 1093615\n",
      "No text found for note_id: 1093583\n",
      "No text found for note_id: 1093356\n",
      "No text found for note_id: 1092919\n",
      "No text found for note_id: 1093408\n",
      "No text found for note_id: 58107\n",
      "No text found for note_id: 38672\n",
      "No text found for note_id: 627175\n",
      "No text found for note_id: 627842\n",
      "No text found for note_id: 627432\n",
      "No text found for note_id: 627442\n",
      "No text found for note_id: 627587\n",
      "No text found for note_id: 627699\n",
      "No text found for note_id: 627621\n",
      "No text found for note_id: 627386\n",
      "No text found for note_id: 627614\n",
      "No text found for note_id: 627241\n",
      "No text found for note_id: 627268\n",
      "No text found for note_id: 627584\n",
      "No text found for note_id: 627615\n",
      "No text found for note_id: 627616\n",
      "No text found for note_id: 627768\n",
      "No text found for note_id: 627625\n",
      "No text found for note_id: 627701\n",
      "No text found for note_id: 627841\n",
      "No text found for note_id: 1127150\n",
      "No text found for note_id: 1127543\n",
      "No text found for note_id: 627582\n",
      "No text found for note_id: 627286\n",
      "No text found for note_id: 627837\n",
      "No text found for note_id: 43313\n",
      "No text found for note_id: 25513\n",
      "No text found for note_id: 41650\n",
      "No text found for note_id: 1200849\n",
      "No text found for note_id: 1201222\n",
      "No text found for note_id: 1201170\n",
      "No text found for note_id: 1201842\n",
      "No text found for note_id: 1200662\n",
      "No text found for note_id: 45238\n",
      "No text found for note_id: 37661\n",
      "No text found for note_id: 35602\n",
      "No text found for note_id: 38450\n",
      "No text found for note_id: 38312\n",
      "No text found for note_id: 42388\n",
      "No text found for note_id: 42674\n",
      "No text found for note_id: 36594\n",
      "No text found for note_id: 41042\n",
      "No text found for note_id: 3457\n",
      "No text found for note_id: 50967\n",
      "No text found for note_id: 46059\n",
      "No text found for note_id: 38112\n",
      "No text found for note_id: 48918\n",
      "No text found for note_id: 53255\n",
      "No text found for note_id: 37210\n",
      "No text found for note_id: 3152\n",
      "No text found for note_id: 45784\n",
      "No text found for note_id: 35665\n",
      "No text found for note_id: 40904\n",
      "No text found for note_id: 34877\n",
      "No text found for note_id: 57952\n",
      "No text found for note_id: 40230\n",
      "No text found for note_id: 42073\n",
      "No text found for note_id: 11299\n",
      "No text found for note_id: 53709\n",
      "No text found for note_id: 37218\n",
      "No text found for note_id: 45120\n",
      "No text found for note_id: 35302\n",
      "No text found for note_id: 32090\n",
      "No text found for note_id: 49145\n",
      "No text found for note_id: 54862\n",
      "No text found for note_id: 46251\n",
      "No text found for note_id: 38406\n",
      "No text found for note_id: 16777\n",
      "No text found for note_id: 37027\n",
      "No text found for note_id: 48799\n",
      "No text found for note_id: 372\n",
      "No text found for note_id: 54476\n",
      "No text found for note_id: 36834\n",
      "No text found for note_id: 36735\n",
      "No text found for note_id: 54285\n",
      "No text found for note_id: 41970\n",
      "No text found for note_id: 14279\n",
      "No text found for note_id: 35053\n",
      "No text found for note_id: 38623\n",
      "No text found for note_id: 23168\n",
      "No text found for note_id: 36132\n",
      "No text found for note_id: 33407\n",
      "No text found for note_id: 47049\n",
      "No text found for note_id: 4951\n",
      "No text found for note_id: 36738\n",
      "No text found for note_id: 46010\n",
      "No text found for note_id: 38851\n",
      "No text found for note_id: 35339\n",
      "No text found for note_id: 50798\n",
      "No text found for note_id: 40745\n",
      "No text found for note_id: 28578\n",
      "No text found for note_id: 22294\n",
      "No text found for note_id: 52801\n",
      "No text found for note_id: 12466\n",
      "No text found for note_id: 40929\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    noteevents_path = Path(r\"C:\\Users\\okechukwu chude\\Documents\\NLP\\text extraction\\Automating-Medical-Coding\\NOTEEVENTS.csv\")\n",
    "    data_dir = Path(r\"C:\\Users\\okechukwu chude\\Documents\\NLP\\text extraction\\Automating-Medical-Coding\\predictions\")\n",
    "    out_dir = Path(r\"C:\\Users\\okechukwu chude\\Documents\\NLP\\text extraction\\Automating-Medical-Coding\\with_text\\gold\")  # Set to None if you don't want to save to an output directory\n",
    "\n",
    "    main(noteevents_path, data_dir, out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################                                   Start DATA.py                              ############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import json\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Iterator, List, Dict, Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataclass for representing a span of text\n",
    "@dataclass(frozen=True)\n",
    "class Span:\n",
    "    begin: int\n",
    "    end: int\n",
    "\n",
    "    covered_text: Optional[str] = dataclasses.field(compare=False, default=None)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.end - self.begin\n",
    "\n",
    "    @staticmethod\n",
    "    def from_note_text(begin: int, end: int, note_text: str):\n",
    "        return Span(begin, end, note_text[begin:end])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataclass for representing a billing code\n",
    "@dataclass(frozen=True)\n",
    "class BillingCode:\n",
    "    code: str\n",
    "    code_system: str\n",
    "    code_description: Optional[str] = dataclasses.field(compare=False, default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataclass for representing an annotation\n",
    "@dataclass(frozen=True)\n",
    "class Annotation:\n",
    "    span: Span\n",
    "    billing_code: BillingCode\n",
    "    type: Optional[str] = dataclasses.field(compare=False, default=None)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_json_dict(data: Dict) -> \"Annotation\":\n",
    "        return Annotation(\n",
    "            span=Span(\n",
    "                begin=data.pop(\"begin\"),\n",
    "                end=data.pop(\"end\"),\n",
    "                covered_text=data.pop(\"covered_text\", None),\n",
    "            ),\n",
    "            billing_code=BillingCode(\n",
    "                data.pop(\"code\"), data.pop(\"code_system\"), data.pop(\"description\", None)\n",
    "            ),\n",
    "            **data,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataclass for representing a note\n",
    "@dataclass(frozen=True)\n",
    "class Note:\n",
    "    note_id: int\n",
    "    category: str\n",
    "    description: str\n",
    "    annotations: List[Annotation] = dataclasses.field(repr=False)\n",
    "\n",
    "    text: Optional[str] = dataclasses.field(repr=False, default=None)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_json_dict(data: Dict) -> \"Note\":\n",
    "        return Note(\n",
    "            text=data.pop(\"text\", None),\n",
    "            annotations=[Annotation.from_json_dict(a) for a in data.pop(\"annotations\")],\n",
    "            **data,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataclass for representing an admission\n",
    "@dataclass(frozen=True)\n",
    "class Admission:\n",
    "    hadm_id: int\n",
    "    notes: List[Note]\n",
    "\n",
    "    comment: str = None\n",
    "\n",
    "    def __iter__(self) -> Iterator[Tuple[Note, Annotation]]:\n",
    "        for note in self.notes:\n",
    "            for annotation in note.annotations:\n",
    "                # Set annotation.span.covered_text if we have note.text\n",
    "                if note.text and not annotation.span.covered_text:\n",
    "                    annotation = dataclasses.replace(\n",
    "                        annotation,\n",
    "                        span=dataclasses.replace(\n",
    "                            annotation.span,\n",
    "                            covered_text=note.text[\n",
    "                                annotation.span.begin : annotation.span.end\n",
    "                            ],\n",
    "                        ),\n",
    "                    )\n",
    "                yield note, annotation\n",
    "\n",
    "    def _has_text(self) -> bool:\n",
    "        \"\"\"Check that all notes have text\"\"\"\n",
    "        return not any(note.text is None for note in self.notes)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_json_dict(data: Dict) -> \"Admission\":\n",
    "        return Admission(\n",
    "            notes=[Note.from_json_dict(note) for note in data.pop(\"notes\")],\n",
    "            **data,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def from_json_file(file_path: Path) -> \"Admission\":\n",
    "        with open(file_path, \"r\", encoding=\"utf8\") as ifp:\n",
    "            return Admission.from_json_dict(json.load(ifp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataclass for representing MDACE data\n",
    "@dataclass(frozen=True)\n",
    "class MDACEData:\n",
    "    admissions: List[Admission]\n",
    "\n",
    "    @property\n",
    "    def __len__(self) -> int:\n",
    "        return sum((1 for _ in iter(self)))\n",
    "\n",
    "    @staticmethod\n",
    "    def from_dir(dataset_dir: Path, require_text: bool = True) -> \"MDACEData\":\n",
    "        try:\n",
    "            next(dataset_dir.glob(\"*.json\"))\n",
    "        except StopIteration:\n",
    "            raise ValueError(f\"No JSON files found in path {dataset_dir.absolute()}\")\n",
    "\n",
    "        admissions = list()\n",
    "        for json_file in dataset_dir.glob(\"*.json\"):\n",
    "            with open(json_file, \"r\", encoding=\"utf8\") as ifp:\n",
    "                adm = Admission.from_json_dict(json.load(ifp))\n",
    "                if require_text and not adm._has_text():\n",
    "                    raise ValueError(\n",
    "                        f\"Admission {adm.hadm_id} is missing note text. \"\n",
    "                        f\"Please run: python inject-note-text.py --noteevents NOTEEVENTS.csv --data-dir {dataset_dir}\"\n",
    "                    )\n",
    "                admissions.append(adm)\n",
    "\n",
    "        return MDACEData(admissions)\n",
    "\n",
    "    def __iter__(self) -> Iterator[Tuple[Admission, Note, Annotation]]:\n",
    "        \"\"\"Iterate over Annotations; include information from Admission and Note\"\"\"\n",
    "        for admission in self.admissions:\n",
    "            for note, annotation in admission:\n",
    "                yield admission, note, annotation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################                                   END DATA.py                              ############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################                                   start text.py                            ############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import re\n",
    "from typing import List, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression pattern to match word tokens\n",
    "TOKEN_PATTERN = re.compile(r\"\\w+\", flags=re.UNICODE | re.MULTILINE | re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> List[Span]:\n",
    "    \"\"\"Tokenizes the given text into word tokens.\"\"\"\n",
    "    \n",
    "    # Find all matches of the token pattern in the lowercase text\n",
    "    matches = TOKEN_PATTERN.finditer(text.lower())\n",
    "\n",
    "    # Create Span objects for each match\n",
    "    spans = [Span(*match.span(), covered_text=match.group()) for match in matches]\n",
    "\n",
    "    # Exclude numbers greater than 10 as per Mullenbach's recommendation\n",
    "    spans = [\n",
    "        span\n",
    "        for span in spans\n",
    "        if not span.covered_text.isdigit() or int(span.covered_text) <= 10\n",
    "    ]\n",
    "\n",
    "    return spans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_annotation(\n",
    "    annotation: Annotation, tokenize_fn: Callable[[str], List[Span]]\n",
    ") -> List[Annotation]:\n",
    "    \"\"\"Tokenizes the covered text of an annotation.\"\"\"\n",
    "    \n",
    "    # Check if the covered text is available for tokenization\n",
    "    if annotation.span.covered_text is None:\n",
    "        raise ValueError(\n",
    "            \"Cannot tokenize annotations without text -- run inject-note-text.py\"\n",
    "        )\n",
    "\n",
    "    token_offset = annotation.span.begin\n",
    "    \n",
    "    # Tokenize the covered text and adjust span positions\n",
    "    return [\n",
    "        dataclasses.replace(\n",
    "            annotation,\n",
    "            span=dataclasses.replace(\n",
    "                span, begin=token_offset + span.begin, end=token_offset + span.end\n",
    "            ),\n",
    "        )\n",
    "        for span in tokenize_fn(annotation.span.covered_text)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_annotations(\n",
    "    annotations: List[Annotation], tokenize_fn: Callable[[str], List[Span]]\n",
    ") -> List[Annotation]:\n",
    "    \"\"\"Tokenizes a list of annotations.\"\"\"\n",
    "    \n",
    "    flat = list()\n",
    "    for a in annotations:\n",
    "        flat.extend(tokenize_annotation(a, tokenize_fn))\n",
    "    return flat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_admission(\n",
    "    admission: Admission, tokenize_fn: Callable[[str], List[Span]]\n",
    ") -> Admission:\n",
    "    \"\"\"Tokenizes all annotations within an admission.\"\"\"\n",
    "    \n",
    "    return dataclasses.replace(\n",
    "        admission,\n",
    "        notes=[\n",
    "            dataclasses.replace(\n",
    "                note, annotations=tokenize_annotations(note.annotations, tokenize_fn)\n",
    "            )\n",
    "            for note in admission.notes\n",
    "        ],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#################################                         text.py end                               #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
